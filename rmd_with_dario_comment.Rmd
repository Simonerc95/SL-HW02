---
title: "rmd_3_4"
author: "Gabriele Tazza"
date: "5/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(require("viridis", quietly = T))
suppressMessages(require("MLmetrics", quietly = T))
set.seed(2020)
```


#### 3. Simulate n = 250 data from the joint data model p(y, x) = p(x|y)Â·p(y) described above:

```{r}
n = 250

#data model

p <- function(x, y){if(y==0){dunif(x, -3, 1) * 1/2}
    else if(y==1){dunif(x, -1, 3) * 1/2}
    else 0}
# curve(p(x,0) + p(x, 1), xlim = c(-5, 5),col = 'red')

ys <- rbinom(n, 1, 0.5)
ys_test <- rbinom(n, 1, 0.5)
gen <- function(y){if(y==0){runif(1, -3, 1)}
    else if(y==1){runif(1, -1, 3)}}
gen <- Vectorize(gen)
xs <- gen(ys)
xs_test <- gen(ys_test)
par(mfrow=c(1,1))
plot.new()
hist(xs, freq = F, col = 'light blue', main = 'Marginal distribution of the variable X')
curve(p(x,0) + p(x, 1),col = 'orange', add = T, lwd = 3)
legend(1.2, 0.3, legend = c('Simulated Histogram', 'Theoretical PDF'), col = c('light blue', 'orange'), 
       lty = 1, lwd = 4, cex=0.8)

```


#### Plot the data together with the regression function that defines $h_{opt}(x)$ and Evaluate the performance of the Bayes Classifiers on these simple

```{r}
# Regression function
plot(xs, ys, col='red', pch=19,  main = 'n = 250 Simulated Data & Regression Function')
reg.fun <- function(x){ p(x, 1)/(p(x,  0) + p(x, 1))}
curve(reg.fun, xlim = c(-5,5), lwd=2, col = 'green', add = T)
legend(-3, 0.9, legend = c('Simulated Data', 'Regression Function'), col = c('red', 'green'), 
       lty = 1, lwd = 4, cex=0.8)

h.opt <- function(x) {if(reg.fun(x) > 0.5) {1}
                     else 0}
h.opt <- Vectorize(h.opt)
accuracy.Bayes = sum(h.opt(xs_test) == ys_test)/length(ys_test)
sprintf("Accuracy of the Bayes Classifier:  %s", accuracy.Bayes)
```

#### Apply any other classifier of your choice to these data and comparatively comment its performance

As another classifier we chose logistic regression one:
```{r}
train= data.frame(X = xs, y =  ys)
test= data.frame(X = xs_test, y =  ys_test)


model = glm(y ~ X, data = train, family = "binomial")
probabilities <- predict(model, newdata=test, type='response')
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
accuracy.Log = sum(predicted.classes == test$y)/length(test$y)
sprintf("Accuracy of the Log classifier:  %s", accuracy.Log)
```

### 4. Repeat the sampling M = 1000 times keeping n = 250 fixed

```{r}
M = 1000
n = 250
total_acc_Bayes = c()
total_acc_Log = c()
for(i in 1:M){
    # generating samples of n=250 observations
    ys <- rbinom(n, 1, 0.5)
    ys_test <- rbinom(n, 1, 0.5)
    xs <- gen(ys)
    xs_test <- gen(ys_test)
    total_acc_Bayes = cbind(total_acc_Bayes,  c(sum(h.opt(xs_test) == ys_test)/length(ys_test)))
    # Logistic Regression
    train= data.frame(X = xs, y =  ys)
    test= data.frame(X = xs_test, y =  ys_test)
    model = glm(y ~ X, data = train, family = "binomial")
    probabilities <- predict(model, newdata=test, type='response')
    predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
    total_acc_Log = cbind(total_acc_Log, c(sum(predicted.classes == test$y)/length(test$y)))
}
avg_acc_Bayes = mean(total_acc_Bayes)
avg_acc_Log = mean(total_acc_Log)
sd_acc_Bayes = sd(total_acc_Bayes)
sd_acc_Log = sd(total_acc_Log)
sprintf("Bayes classifier, Mean:  %s, Std: %s ", avg_acc_Bayes, sd_acc_Bayes)
sprintf("Log classifier, Mean:  %s, Std: %s ", avg_acc_Log, sd_acc_Log)

```

The accuracy of both the models are very close, even thought the performance of the Bayes Classifier is slightly higher then the one of the Logistic Regression.
This may due by the fact that in this setup the main source of error is the irreducible part, since in the intervals $[-1, 1]$ the model doesn't have any information for discriminate between the two classes. <br>
Then, in one end we have the Bayes Classifier that was realized knowing the data generating process, so having as error only the irreducible component, and on the other end the Logistic Regression that approximate to the same solution of the Bayes Classifier, but with a small approximation error that gets added to the irreducible component.
