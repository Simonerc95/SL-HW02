---
title: "Homework 2"
author: "Ruggeri D., Ercolino S., Tazza G."
date: "25/5/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(require("viridis", quietly = T))
set.seed(2020)
```

## Exercise 1: The Bayes Classifier

### 1. Bayes strategy

### 2. Bayes Classification Rule $h_{opt}(x)$
The Bayes classification rule $h_{opt}(x)$ for a binary classification (s.t. $Y \in \mathcal{Y} = \{0,1\}$) is given by:

$$
h_{opt}(x) = \mathbb{I}\left[\mathbb{P}(Y = 1 ~ | ~\mathbf{X} = \mathbf{x}) > \frac{1}{2}\right]
$$
Where (simplifying the notation for the $\mathbf{X}$), we have the Regression Function:
$$
\mathbb{P}(Y = 1 ~ |~ \mathbf{X}) = (Bayes)= \frac{ \mathbb{P}(\mathbf{X} ~ | ~Y=1) \cdot \mathbb{P}(Y=1)}{\mathbb{P}(\mathbf X)} =   \frac{ \mathbb{P}(\mathbf{X} ~ | ~Y=1) \cdot \mathbb{P}(Y=1)}{\mathbb{P}(\mathbf X ~ | ~Y=1)\cdot\mathbb{P}(Y=1) + \mathbb{P}(\mathbf X ~ | ~Y=0)\cdot\mathbb{P}(Y=0)}
$$

Since we assume $(X,Y)$ r.v. with $Y \in \{0,1\}$ and $X \in \mathbb{R}$, having conditional distribution:
$$
(X ~|~Y = 1) \sim Unif(-1, 3) ~~~ and~~~ (X ~|~Y = 0) \sim Unif(-3, 1)
$$
and:
$$
\mathbb{P}(Y=1)=\mathbb{P}(Y=0)= \frac{1}{2}
$$ 
But, since the conditional distributions of $X$ are uniforms, we can rewrite the probabilities as:
$$
\mathbb{P}(X ~|~Y = 1) = \frac{1}{4} \mathbb{I}_{[-1,3]} (x) ~~ and ~~ \mathbb{P}(X ~|~Y = 0) = \frac{1}{4} \mathbb{I}_{[-3,1]} (x)  
$$

So, by substituting the values, the Regression Function $\mathbb{P}(Y=1|X)$ for this classification problem becomes:

$$
\mathbb{P}(Y=1~|~X) = \frac{\frac{1}{8} \mathbb{I}_{[-1,3]} (x)}{\frac{1}{8} \mathbb{I}_{[-3,1]}(x) + \frac{1}{8} \mathbb{I}_{[-1,3]}(x)} =  \frac{ \mathbb{I}_{[-1,3]}(x)}{ \mathbb{I}_{[-3,1]}(x) +  \mathbb{I}_{[-1,3]}(x)} = \frac{ \mathbb{I}_{[-1,1)}(x) + \mathbb{I}_{[1,3]}(x)}{ \mathbb{I}_{[-3,-1]}(x) + 2 \mathbb{I}_{[-1,1]}(x) +  \mathbb{I}_{[1,3]}(x)}  = 
$$
$$
 = \left\{\begin{array}{ll}
                  0 ~~ if ~~ -3\leq x < -1\\
                  \frac{1}{2} ~~ if ~~ -1\leq x < 1\\
                  1 ~~ if ~~ 1\leq x\leq 3
                \end{array}
              \right. \implies \\
$$
$$
 \implies \mathbb{P} (Y=1~|~X) = \frac{1}{2} \mathbb{I}_{[-1,1)} + \mathbb{I}_{[1,3]}
$$

And the classification rule $h_{opt}$ can be written as:
$$
h_{opt}(x) = \mathbb{I}_\left[\mathbb{P}(Y = 1 ~ | ~ X) > \frac{1}{2}\right] (x) = \mathbb{I}_\left[ \frac{1}{2} \mathbb{I}_{[-1,1)} + \mathbb{I}_{[1,3]} > \frac{1}{2}\right] (x) = \mathbb{I}_{[1, 3]} (x)
$$
Which means that the observations are classified as $y=1$ when the feature $X$ has values in $[1,3]$, and $y=0$ otherwise.
